# Temporal Difference Learning

If one had to identify one idea as central and novel to RL, it would undoubtedly be temporal-difference(TD) learning. TD learning is a combination of Monte Carlo ideas and dynamic programming (DP) ideas. Like Monte Carlo methods, TD methods can learn directly from raw experience without a model of the environmentâ€™s dynamics. Like DP, TD methods update estimates based in part on other learned estimates, without waiting for a final outcome (they bootstrap).

Here are test and comparison results of some simple methods used in TD under Reinforcement Learning(RL).</br>

### 1.Random Walk


run the code(as per the correct file path):
>python3 random_walk.py

#### Results are as follows: 

<img src="result_images/Figure_1.png" alt="" width="420"/><img src="result_images/Figure_2.png" alt="" width="420"/>
